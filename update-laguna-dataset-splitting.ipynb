{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e10c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c718c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "# Define the source path\n",
    "dataset_path = os.path.join(os.path.expanduser('~'), 'Desktop', 'laguna_dataset')\n",
    "\n",
    "# Define the directory path for preprocessing\n",
    "dataset_preprocessing_path = os.path.join(dataset_path, 'laguna_dataset_preprocessed')\n",
    "\n",
    "# Check if the preprocessing directory and CSV file already exist\n",
    "if os.path.exists(dataset_preprocessing_path) and os.path.exists(os.path.join(dataset_preprocessing_path, 'filtered_metadata.csv')):\n",
    "    print(\"Preprocessing directory and metadata file already exist. Skipping data preprocessing.\")\n",
    "else:\n",
    "    # Create the directory for dataset preprocessing inside the working directory\n",
    "    os.makedirs(dataset_preprocessing_path, exist_ok=True)\n",
    "\n",
    "    # Load metadata from the CSV file\n",
    "    metadata_file_path = os.path.join(dataset_path, 'metadata.csv')  # Replace with your actual CSV file path\n",
    "    if os.path.exists(metadata_file_path):\n",
    "        metadata = pd.read_csv(metadata_file_path)\n",
    "    else:\n",
    "        print(\"Metadata file not found. Please provide the correct path to your metadata file.\")\n",
    "        exit()\n",
    "\n",
    "    # Define the specific diseases and parts included\n",
    "    diseases = ['Healthy', 'Bunchy top', 'Black sigatoka']\n",
    "    parts = ['foliage', 'leaf']\n",
    "\n",
    "    # Filter out rows with 'treeID' containing 'Unrecorded'\n",
    "    metadata = metadata[metadata['treeID'] != 'Unrecorded']\n",
    "\n",
    "    # Shuffle the metadata to distribute the data randomly\n",
    "    metadata = metadata.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Initialize an empty list to store the metadata of the filtered images\n",
    "    filtered_metadata = []\n",
    "\n",
    "    # Iterate through the metadata, apply the \"disease,\" \"part,\" and \"treeID\" filters, and move the images\n",
    "    for _, row in metadata.iterrows():\n",
    "        imageID = row['imageID']\n",
    "        disease = row['disease']\n",
    "        part = row['part']\n",
    "        treeID = row['treeID']\n",
    "\n",
    "        if disease in diseases and part in parts and treeID != 'Unrecorded':\n",
    "            # Remove the \".jpg\" extension from imageID if present\n",
    "            imageID = imageID.replace(\".jpg\", \"\")\n",
    "\n",
    "            # Add the \".jpg\" extension to the source_path\n",
    "            source_path = os.path.join(dataset_path, f\"{imageID}.jpg\")\n",
    "            dest_path = os.path.join(dataset_preprocessing_path, f\"{imageID}.jpg\")\n",
    "            shutil.move(source_path, dest_path)\n",
    "\n",
    "            # Append the filtered metadata to the list\n",
    "            filtered_metadata.append(row)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    filtered_metadata = pd.DataFrame(filtered_metadata)\n",
    "\n",
    "    # Save the filtered metadata to a CSV file inside the dest_path\n",
    "    csv_file_path = os.path.join(dataset_preprocessing_path, 'filtered_metadata.csv')\n",
    "    filtered_metadata.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5622d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check treeIDs with duplicate and count it\n",
    "# Define the path to the 'filtered_metadata.csv' file\n",
    "filtered_metadata_path = os.path.join(dataset_preprocessing_path, 'filtered_metadata.csv')\n",
    "\n",
    "# Check if the filtered_metadata.csv file exists\n",
    "if os.path.exists(filtered_metadata_path):\n",
    "    # Load the 'filtered_metadata.csv' file into a DataFrame\n",
    "    filtered_metadata = pd.read_csv(filtered_metadata_path)\n",
    "\n",
    "    # Calculate the count of duplicates for each treeID\n",
    "    treeID_duplicate_counts = filtered_metadata['treeID'].value_counts()\n",
    "\n",
    "    # Display the treeID and its count of duplicates\n",
    "    print(\"treeID - Count of Duplicates:\")\n",
    "    for treeID, count in treeID_duplicate_counts.items():\n",
    "        print(f\"{treeID} - {count}\")\n",
    "\n",
    "    # Check if the sum of duplicate counts is 883\n",
    "    total_duplicates = treeID_duplicate_counts.sum()\n",
    "    print(f\"Total Duplicate Counts: {total_duplicates}\")\n",
    "    if total_duplicates == 883:\n",
    "        print(\"The sum of duplicate counts is equal to 883.\")\n",
    "    else:\n",
    "        print(\"The sum of duplicate counts is not equal to 883.\")\n",
    "else:\n",
    "    print(\"The 'filtered_metadata.csv' file does not exist. Please make sure the file is in the correct location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b37685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset Splitting \n",
    "# Define the diseases (class labels)\n",
    "diseases = ['Healthy', 'Black sigatoka', 'Bunchy top']\n",
    "\n",
    "# Create directories for train, valid, and test datasets\n",
    "train_dir = os.path.join(dataset_preprocessing_path, 'train')\n",
    "valid_dir = os.path.join(dataset_preprocessing_path, 'valid')\n",
    "test_dir = os.path.join(dataset_preprocessing_path, 'test')\n",
    "\n",
    "# Create subdirectories for each disease in train, valid, and test directories\n",
    "for subset_dir in [train_dir, valid_dir, test_dir]:\n",
    "    for disease in diseases:\n",
    "        os.makedirs(os.path.join(subset_dir, disease), exist_ok=True)\n",
    "\n",
    "# Rest of your code for splitting and copying images\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Shuffle the unique treeIDs to ensure randomness\n",
    "unique_treeIDs = filtered_metadata['treeID'].unique()\n",
    "shuffled_treeIDs = unique_treeIDs.copy()\n",
    "random.shuffle(shuffled_treeIDs)\n",
    "\n",
    "# Initialize dictionaries to keep track of image allocations\n",
    "image_allocations = {treeID: None for treeID in shuffled_treeIDs}\n",
    "allocated_counts = {'train': 0, 'valid': 0, 'test': 0}\n",
    "\n",
    "# Split and move images to their respective subsets\n",
    "for treeID in shuffled_treeIDs:\n",
    "    # Determine the subset for the current treeID\n",
    "    if allocated_counts['train'] / len(shuffled_treeIDs) < train_ratio:\n",
    "        subset = 'train'\n",
    "    elif allocated_counts['valid'] / len(shuffled_treeIDs) < valid_ratio:\n",
    "        subset = 'valid'\n",
    "    else:\n",
    "        subset = 'test'\n",
    "\n",
    "    # Get all images with the current treeID\n",
    "    treeID_images = filtered_metadata[filtered_metadata['treeID'] == treeID]\n",
    "\n",
    "    for index, row in treeID_images.iterrows():\n",
    "        imageID = row['imageID']\n",
    "        imageID = imageID.replace(\".jpg\", \"\")  # Remove the \".jpg\" extension\n",
    "        disease = row['disease']  # Assumes 'disease' column in the DataFrame\n",
    "\n",
    "        # Construct the source and target paths\n",
    "        image_path = os.path.join(dataset_preprocessing_path, f\"{imageID}.jpg\")\n",
    "        target_dir = os.path.join(dataset_preprocessing_path, subset, disease, f\"{imageID}.jpg\")\n",
    "\n",
    "        # Check if the source image exists\n",
    "        if os.path.exists(image_path):\n",
    "            print(f\"Moving {imageID} to {target_dir}\")\n",
    "            shutil.move(image_path, target_dir)\n",
    "        else:\n",
    "            print(f\"Source image {imageID} not found at {image_path}\")\n",
    "\n",
    "    # Update allocation and allocated counts\n",
    "    allocated_counts[subset] += 1\n",
    "\n",
    "print(\"Data splitting and copying completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if Images from the same treeID is only in one subset (i.e., Train, Val, or Test).\n",
    "# Define the diseases (class labels)\n",
    "diseases = ['Healthy', 'Black sigatoka', 'Bunchy top']\n",
    "subsets = ['train', 'valid', 'test']\n",
    "\n",
    "# Iterate through subsets and diseases\n",
    "for subset in subsets:\n",
    "    for disease in diseases:\n",
    "        folder_path = os.path.join(dataset_preprocessing_path, subset, disease)\n",
    "        images_in_folder = os.listdir(folder_path)\n",
    "\n",
    "        # Filter data from filtered_metadata.csv for images in the folder\n",
    "        matching_data = filtered_metadata[filtered_metadata['imageID'].isin(images_in_folder)]\n",
    "\n",
    "        print(f\"TreeID counts in {subset}/{disease} folder:\")\n",
    "        treeID_counts = matching_data['treeID'].value_counts()\n",
    "        for treeID, count in treeID_counts.items():\n",
    "            print(f\"- TreeID {treeID}: {count} times\")\n",
    "\n",
    "print(\"Counts printed for each folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
